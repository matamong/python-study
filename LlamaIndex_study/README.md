# 라이브러리 위치
- https://github.com/run-llama/llama_index

<br><br>

# 용어 정리 (헷갈리지 않게!)
- **벡터의 크기/길이(norm)**:  
  - 원점 (0,0,...)에서 **벡터 하나**까지의 거리  
  - `||v||` 로 표기
- **(벡터 사이) 거리(distance)**:
  - 벡터 A와 벡터 B, **두 점 사이의 거리**  
  - `||A - B||` 로 표기
- **축**
  - 실제 임베딩 공간에서는 각 축이 이런 식의 추상적인 feature 방향이라고 볼 수 있다:
    - 어떤 축은 “긍정 감성”
    - 어떤 축은 “기술/논문 스타일”
    - 어떤 축은 “캐주얼함/구어체 느낌”
    - 어떤 축은 “추천/선호도” …
    - ...
  - 축의 개수는 모델 설계자가 정하고, 축이 실제로 무슨 의미 방향인지는 임베딩 모델이 알아서 결정함
  - 축은 곧 차원이 됨
  - 참고로 openAI 임베딩 모델들 기준으로 `text-embedding-3-small` 은 축이 1536개, `text-embedding-3-large` 는 3072개다. 제미나이는 임베딩은 768, 1536, 3072까지 됨.
- **축과 길이**
  - 한 문장의 임베딩 벡터를 보면:
    - 각 좌표값 = 해당 feature 축 방향으로 얼마나 갔는지 (얼마나 켜졌는지)
    - 벡터의 크기(길이, norm) = 이 값들을 한꺼번에 모아서 본 전체적인 “세기” 정도로 볼 수 있다.
    - 좌표들이 전반적으로 크면
      - 여러 feature가 강하게 나타난 문장일 수 있고
    - 좌표들이 전반적으로 작으면
      -  전체적으로 약한/평이한 문장일 수 있다.
   - **단, 이 해석은 모델마다 다르고, 길이가 “확신도”를 반영하기도 하고 아무 의미 없는 스케일일 수도 있다. 그래서 실무에서는 아예 크기를 1로 맞춰버리고(정규화) 각도만 보는 경우도 많다.**
- **norm**
  - 벡터에 길이(크기)를 매기는 규칙
  - 왜 길이라고 안하고 norm이라고 부르냐면, 
    - 2차원, 3차원에서의 길이는 우리가 아는 피타고라스 길이를 사용할 수 있어서 걍 길이라고 하면 됨.
    - 근데, 머신러닝이나 수학 쪽에서는 벡터 말고도 이상한 공간들(함수, 행렬, 고차원 등) 에서도 길이 비슷한 것을 정의해야 함
    - 그래서 그냥 길이라고 부르긴 좀 애매해서 norm이라고 통일해서 부르는 것.
  - 사실 임베딩/벡터 얘기할 때는 norm이나 길이나 크기나 magnitude 같다고 생각해도 문제 없음.
  - 같은 벡터라도, 어떤 norm을 쓰느냐에 따라서 "길이"가 달라질 수 있음.
    - L1 norm
      - 절댓값의 합, 맨하탄 거리(Manhattan distance) 
    - L2 norm
      - 피타고라스 길이
<br><br>

# L1, L2 정규화 개념 정리

## 1. 기본 세팅: 2D 평면 + 벡터 하나

벡터 하나 잡자:

* (x = (3, 4))

```text
      y ↑
    5 |        *
    4 |      x (3,4)
    3 |
    2 |
    1 |
    0 +----------------→ x
      0  1  2  3  4  5
```

이 (3,4) 점까지의 **화살표**가 벡터라고 생각하면 됨.

이때:

* **L2 길이(크기)** = 피타고라스 길이
  → √(3² + 4²) = 5
* **L1 길이(합)** = 절댓값 합
  → |3| + |4| = 7

---

## 2. L2 정규화: “원 위로 끌어내리기”

**L2 정규화=“길이를 1로 맞추기”**임.
(방향은 그대로, 길이만 1 되게 줄이기)

### 2-1. 2D에서 L2 = 원(circle)

“길이가 1인 벡터들”의 집합은 **원**이다:

```text
      y ↑

    1 |      * (0,1)
      |   .     .
      | .         .
    0 +*-----------*→ x
     -1  .       .  1
        .   . .   .
          * (0,-1)

(대충 반지름 1짜리 원)
```

* (1,0), (0,1), (-1,0), (0,-1) 같은 점들이 **L2 길이 = 1**

### 2-2. 우리 벡터 (3,4)를 원 위로!

원래 벡터:

* (3,4), L2 길이 = 5

L2 정규화:

* (3/5, 4/5) = (0.6, 0.8)
* 이 점은 “길이 1짜리 원” 위에 있게 됨.

ASCII 감으로 그리면:

```text
      y ↑
    1 |          * (0,1)
  0.8|        x̂ (0.6,0.8)
  0.6|
    0 +--------------------→ x
       0    0.6     1
```

느낌:

* 원래 (3,4)는 더 멀리 있었는데
* **같은 방향**으로 **슉 줄여서** 원 위로 올려놓는 거 = L2 정규화

> 👉 **L2 정규화 요약**
> “벡터를 자기 L2 길이로 나눠서,
> **길이 1짜리 화살표**로 만드는 것”

---

## 3. L1 정규화: “마름모(다이아) 위로 끌어내리기”

이번엔 **L1 정규화**.

### 3-1. 2D에서 L1 = 마름모(다이아몬드)

“|x| + |y| = 1”인 점들의 집합이 바로 **L1 노름 = 1** 인 점들임.

그려보면 이런 모양:

```text
      y ↑
        |
    1   * (0,1)
        |
        |
        |
-1 -----*----- 1 → x
        |
        |
        |
    -1  * (0,-1)
```

조금 더 디테일하게 보면:

```text
      y ↑
        |
    1   * (0,1)
        |\ 
        | \
        |  \
        |   \
-1 -----*----*---- 1 → x
        |   /
        |  /
        | /
    -1  * (0,-1)

   (|x| + |y| = 1 이 이루는 마름모)
```

코너 네 개:

* (1,0), (0,1), (-1,0), (0,-1)

이 점들은 L1 길이:

* |1|+|0| = 1
* |0|+|1| = 1  … 라서 **L1 = 1**

### 3-2. (3,4)를 L1 정규화하면?

원래: x = (3,4), L1 길이 = 7
L1 정규화:

* (\tilde{x} = (3/7, 4/7) ≈ (0.4286, 0.5714))
* 이 점은 **|0.4286| + |0.5714| ≈ 1**
  → 마름모 위로 내려옴

대충 그림:

```text
      y ↑
    1 |      * (0,1)
 0.8 |
 0.6 |        x̃ (≈0.43, 0.57)
 0.4 |
 0.2 |
    0 +--------------------→ x
       0   0.4   0.8   1
```

느낌:

* (3,4)를 **L1 길이가 1 되도록** 줄인 버전

> 👉 **L1 정규화 요약**
> “벡터를 자기 L1 길이(|x₁|+…+|xₙ|)로 나눠서,
> **절댓값 합이 1인 벡터**로 만드는 것”
> (그래서 종종 “비율/분포”처럼 해석하기 좋음)

---

## 4. L1 vs L2를 딱 ASCII로 비교

똑같이 (3,4)라는 점이 있을 때:

```text
      y ↑
    1 |       * (0,1)
      |    L2:   x̂ ≈ (0.6, 0.8)
      |   L1: x̃ ≈ (0.43,0.57)
      |
    0 +----------------------→ x
       0   0.4   0.6    1
```

* 둘 다 원래 (3,4)에서 **방향은 비슷하게** 줄어들었는데,
* L2는 “원 위로”,
* L1은 “마름모 위로” 올라감.

그래서:

* **L2 정규화**:

  * “길이 = 1”인 **원(원통형 구)** 위에 점들이 놓임
  * 코사인/유클리드와 궁합 좋음 → 임베딩에서 제일 자주 씀

* **L1 정규화**:

  * “좌표 절댓값 합 = 1”인 **마름모/단체(simplex)** 위에 점들이 놓임
  * “각 축의 비율” 같은 느낌을 줄 때 좋음 (확률분포, 카운트 비율 등)

---

## 한 줄 요약

* **L2 정규화**

  * `x / ||x||₂`
  * “벡터를 **길이 1짜리 화살표**로 만든다”
  * 원 / 구(surface of sphere) 위에 점들이 놓임

* **L1 정규화**

  * `x / ||x||₁`
  * “벡터를 **절댓값 합이 1인 벡터**로 만든다”
  * 마름모 / 다이아몬드 / 단체(simplex) 위에 점들이 놓임


<br><br>


# 벡터 유사도
2 차원 공간에 두 문장의 임베딩 벡터가 있다고 상상해보자.
- `벡터 A`: "강아지가 귀여워" → [3, 4]
- `벡터 B`: "고양이가 사랑스러워" → [4, 3]
```text
    ↑ (y축)
  5 |
  4 |    A(3,4)
  3 |       B(4,3)
  2 |
  1 |
  0 |________→ (x축)
    0 1 2 3 4 5
```
여기서 A와 B는 좌표가 약간 다르지만,
둘 다 “(대략) 오른쪽 위를 향하는 벡터”라는 의미에서
방향이 비슷한 벡터라고 볼 수 있다.

### 각도는 같고, 크기만 다른 경우(개념 그림)

아래는 “각도(방향)는 같고 크기만 다른” 상황을 개념적으로 표현한 그림이다.
```
각도는 같다.
        ↑
        |
        |  /  방향은 같은데
        | /   길이(크기)만 다름
        |/
        •────→
```
### 크기는 다를 수 있다.
```
        ↑
      8 |       • C(6,8) "매우 긍정적"
      6 |     /
      4 |    • B(3,4) "긍정적"  
      2 |  /
      0 •────────→
        0   ..  8
```
둘은 같은 방향(각도)이지만, C가 더 긴 벡터이다. 즉, 방향(의미 패턴)은 같은데 “강도/세기”만 두 배라고 볼 수 있다.


## Cosine Similarity (각도 기반)
**계산법**: 
- cos(θ) = (A·B) / (|A| × |B|)
```
A·B = 3×4 + 4×3 = 24
|A| = √(3²+4²) = 5
|B| = √(4²+3²) = 5
cosine = 24 / (5×5) = 0.96
```

<br>
- numpy로는
  
```python
product = np.dot(embedding1, embedding2)
norm = np.linalg.norm(embedding1) * np.linalg.norm(embedding2)

result = product / norm
```
- 그냥 공식임. 이거 Dot Product한거에서 길이가 필요없어서 빼버리는 느낌인데 이거는 복습할 때 이 문서 구조상 이해안될 수 있으니 쭉쭉 읽고 다시 와보면 이해가 될 듯. 고로 이해 안되면 일단 넘어가~

**특징**:
- 벡터 간의 **`각도`** 만 본다. (방향성만 본다는 얘기)
- 벡터의 크기는 무시
- 결과가 -1 ~ 1 (1에 가까울 수록 유사)

**언제?**:
- 문서 길이가 달라도 의미가 비슷하면 유사하다고 판단하고 싶을 때
  - 짧은 트윗 vs 긴 블로그 글
    - 같은 주제면 유사하다고 봄

## Dot Product (각도 + 크기 기반)
**계산법**:
-  A·B = A₁×B₁ + A₂×B₂
```
3×4 + 4×3 = 24
```

<br>

- numpy로는

```python
result = np.dot(embedding1, embedding2)

# np.dot은 즉 이렇게 계산 함.
a = [a1, a2, a3]
b = [b1, b2, b3]

a·b = a1*b1 + a2*b2 + a3*b3

```
- 벡터끼리 곱해서 거리가 양수(가까운거)는 확대하고, 거리가 음수나 0이면(먼거)는 축소해버리는 것.
- 즉, 같이 크게 켜진 차원들을 보상해 주고, 서로 다르게 켜지거나 반대인 차원들은 감점시키는 구조.

**특징**:
- 각도 + 벡터의 크기(magnitude) 둘 다 고려
- 정규화 안 된 벡터에선 큰 벡터가 더 높은 점수를 받음
- 결과가 음수나 양수

**언제?**:
- 임베딩 모델이 이미 정규화(L2) 되어 있고, 약간의 성능 이득이 필요할 때
- 벡터 크기 자체가 중요한 의미를 가질 때
  - 매우 긍정적(큰 벡터) vs 조금 긍정적(작은 벡터) 를 구분하고 싶을 때
  - 추천 시스템에서 선호도 강도를 반영하고 싶을 때

**참고**:
- 모델이 어떻게 학습됐냐에 따라 길이가 “확신도”를 반영하기도 하고 그냥 아무 의미 없는 스케일일 수도 있어서,
- 그래서 실무에서는 아예 길이를 1로 맞춰버리고(정규화) 각도만 보기도 함.

## Euclidean Distance (거리 기반)
**계산법**:
- distance = √((A₁-B₁)² + (A₂-B₂)²)
```
# √((3-4)² + (4-3)²) = √(1 + 1) = 1.41
```
**특징**:
- 두 점 사이의 직선 거리
- 작을수록 유사함 (0에 가까울수록 가까움)
- 공간상의 **실제 거리**를 측정

**계산법**:
- 임베딩 공간에서 "얼마나 멀리 떨어져 있나"가 중요할 때
- 클러스터링에서 같은 그룹인지 판단할 때
  - 얼굴 인식에서 "이 사람이 맞나" 판단할 때
  - 이상치 탐지  

## 예시
### 다른 각도, 비슷한 크기
```
        ↑
      5 |  • D(0,5) "슬픔"
        |  |
      3 |  |
        |  |
      0 •──────→
        0  3  5
           |
           • E(5,0) "분노"

각도: 90도 차이 (완전히 다른 방향)
크기: 둘 다 5 정도로 비슷
```
- Cosine
  - 각도가 90도니까 0으로 전혀 안 비슷
- Dot Product
  - 0x5 + 5x0 = 0으로 전혀 안 비슷
- Euclidean
  - 거리 재니까 7.07로 꽤 멀리 떨어짐

### 비슷한 각도, 다른 크기
```
        ↑
      4 |     • "AI는 정말 대단해요!!!" (4,3)
      3 |    /•  "AI 괜찮네" (2,1.5)
      2 |   /
      1 |  /
      0 •────────→
        0 1 2 3 4

두 문장 모두 "AI 긍정" 방향
하지만 감정의 '강도'가 다름
```
- Cosine
  - 방향이 거의 같으니 의미는 같다고 생각
- Dot Product
  - 방향은 같지만, 크기를 반영해서 차이가 남
- Euclidean
  - 2.5만큼 떨어져 있으니 감정 강도가 차이가 난다고 판단


# 유클리드 유사성 VS 코사인 유사성
## 코사인은 벡터 각도, 유클리드는 두 점 사이의 거리
예를 들어, 이커머스 환경에서 제품 추천을 위해 사용자를 비교하고 싶다고 해보자.
- `사용자1` 은 계란 1개, 밀가루 1개, 설탕 1개를 구매
- `사용자2`는 계란 100개, 밀가루 100개, 설탕 100개를 구매
- `사용자3`은 계란 1개, 보드카 1개, 레드불 1개를 구매

상품 축은 이렇게 잡았다고 칩시다.
- 1축: 계란
- 2축: 밀가루
- 3축: 설탕
- 4축: 보드카
- 5축: 레드불

그러면 사용자들은 벡터로 이렇게 표현 될 수 있음
- 사용자1: 계란 1, 밀가루 1, 설탕 1
  - u1 = `(1, 1, 1, 0, 0)`
- 사용자2: 계란 100, 밀가루 100, 설탕 100
  - u2 = `(100, 100, 100, 0, 0)`
- 사용자3: 계란 1, 보드카 1, 레드불 1
  - u3 = `(1, 0, 0, 1, 1)`

```
y (파티재료)
↑
4 |            u3(1,2)
3 |             *
2 |           *
1 |         
0 |*------------------------------→ x (빵재료)
  |u1(3,0)                 u2(300,0)
  0   1   3           ...       300
```

### 감으로 보면:
- u1, u2 → 빵/디저트 재료 덕후 (종류는 같고 양만 다름)
- u3 → 파티 재료(보드카+레드불) + 계란 (완전 다른 타입)

### 코사인 유사도에 따르면:
```
y
↑
|      (둘 다 y=0 위에 있음)
| u1 *----------* u2
+------------------------→ x
  0        3        300
```
- u2는 u1이랑 완전 같은 방향이고, 그냥 길이만 100배 늘어난 벡터임.
- 고로 **`사용자1` 과 `사용자2`가 유사하다.**
- 양만 다를 뿐, 계란, 밀가로, 설탕 들의 재료(축이 될 수 있음)가 같음
- 패턴(방향) 위주임


### 유클리드 유사도에 따르면
```
y
↑
0 |*---------------------------------*→ x
  |u1(3,0)                      u2(300,0)
  0    3                  ...      300
```
```
y
↑
4 |         u3(1,2)
3 |          *
2 |        *
1 |
0 |   *----------------------→ x
    u1(3,0)
```
  - **`사용자3`은 `사용자1`과 더 유사하다.**
  - 양 차이가 너무 심한 것보다 양 차이가 적은 걸 더 유사하다고 판단.
  - 전체 규모까지 같이 봄

참고:  https://datascience.stackexchange.com/questions/27726/when-to-use-cosine-simlarity-over-euclidean-similarity

<br><br>

# 차원의 저주 (Curse of Dimensionality)
벡터, 거리, 임베딩은 모두 고차원에서 돌아가게 됨. (768차원....) <br>
그래서, 차원이 높아질수록 가깝다, 비슷하다, 밀집됐다같은 감각이 전부 박살나면서 생기는 온갖 문제들을 묶어서 `차원의 저주` 라고 부름.

## 공간이 너무 빨리 커진다 -> 데이터가 허허벌판에 흩어짐

1차원일 때는
```
0────────────────────1
```
여기에 데이터 10개만 뿌려놔도 괜찮음.

2차원일 때는
```
(0,1)
  +-----------------+
  |   ·   ·         |
  |      ·          |
  |  ·        ·     |
  |      ·   ·      |
  +-----------------+ (1,0)
(0,0)
```
10개를 뿌려놨을 때 1차원보다 드문드문해짐

3차원일 때는 상자 하나 안에 점 10개가 떠다니는 느낌이라 더 허전해짐 <br>
```
대충 어린왕자마냥 박스 안에 점이 있음 ^^

       z
       ↑
      +------+
     /      /|
    +------+ |
    |      | +
    |      |/
    +------+
 (0,0,0)   → x
```

이걸 임베딩 차원인 768차원으로 생각해보면 공간 크기가 차원 수에 대해 폭발해서 데이터를 넣어도 텅 비어 보이는 상태가 됨. 그래서
- **같은 밀도를 유지하려면 차원이 1 늘어날 때마다 몇 배씩 더 많은 데이터가 필요**
- **그래서 고차원일수록 데이터가 희소(sparse)해지고, 비슷한 점을 찾기가 어려워진다!!**

## 가까운거나 먼거나 다 비슷해 보임 - 거리의 붕괴(집중)
2차원이나 3차원에서는 가까운 점은 가깝고 먼 점은 진짜 먼게 잘 느낌이 난다. <br>
근데 차원이 올라가면 이런 느낌들이 이상해진다! 그래서 서로 크게 차이가 안 나게 된다.<br>

2차원에서는
```
  ·     (점들이 어떤 건 확 가깝고, 어떤 건 멀고)
      ·

   ·       ·
```
이 점들의 거리 차이가 눈으로 보인다.
그런데 고차원에서는 가장 가까운 놈이나 가장 먼 놈이나 거리가 다 비슷비슷해진단다. 이걸 **distance conventration**이라고 한다고 함. <br>
법칙의 큰수(LLN)이랑 중심극한정리(CLT)라는 통계 이야기라는데 뭐라는지 모르겠어서... <br>
대충, 거리를 잴 때 공식으로 평균(차원수 × (차원별 차이²의 평균))을 구하는데, <br>

> **시험 과목 2개일 때는 과목 하나 망치면 총점이 요동치는데, 100과목일 때는 과목 하나 망쳐도 전체 평균은 크게 안 변하는 느낌**

이런 느낌이라고 한다. <br>
**임베딩 검색** 에서는 이게 좀 아픈게, 768차원에서 L2 거리를 쓰면 차이가 작아져서 좀 까다로워짐. (이럴 때는 ANN, metric 선택, 정규화등이 중요해짐)



## 안 쪽은 텅 비고, 대부분이 경계에 몰린다.
하이퍼큐브(0~1 구간을 n차원으로 곱한 공간)를 상상해보자.
- 1D: [0,1] 선분
- 2D: [0,1]×[0,1] 정사각형
- 3D: [0,1]×[0,1]×[0,1] 정육면체
- …
- nD: [0,1]^n 하이퍼큐브

여기서 “중앙 근처” vs “테두리 근처”를 비교하면, 차원이 높아질수록 대부분의 점이 경계(바깥쪽)에 몰려 있다는 게 수학적으로 증명이 된다. (어케했어!) <br>
그래서 대충 **고차원일수록 안쪽은 비고, 경계 근처에 점들이 분포함.**

## 이게 왜 저주일까
이게 왜 저주냐면, 
### 데이터 희소(sparse)
- 차원이 높아질 수록 훨씬 더 많은 데이터가 필요해짐 (공간이 너무 빨리 퍼져서)
### 거리 기반 알고리즘이 힘들어짐
- K-NN, 클러스터링(k-means) 같은 것들로 가깝거나 먼 것들을 잘 써먹어야하는데, 고차원에서는 그게 분명하지가 않음

### 연산량 폭발
- 차원이 n이고 데이터가 n개면 연산량이 폭발함!!
  - 거리 한 번 재는데 O(n)이고 모든 쌍 비교하는데 O(N²n) 이 됨...! 
- 그래서 FAISS나 HNSW같은 ANN(Approximate Nearest Neighbor, 근사치 최근접 이웃)이 나온 것
  - ANN: 반드시 가장 가까운 포인트가 아니라 충분히 가까운 근사치를 빠르게 찾아내는 알고리즘임.

### 그래서 하는 것들
이 차원의 저주를 해결하기 위해서 실무에서 아래를 하고 있다고 한다. 어우 공부할 것 많다.
- **차원 줄이기**
  - PCA, SVD, autoencoder, projection 등등
- **정규화, metric 선택**
  - cosine vs dot vs L2 등등..
- **Approximate Nearest Neighbor(ANN) 인덱스 쓰기**
- **regularization, dropout 같은 걸로 실질적으로 유효 차원 줄이기**

넘 재밌고만!!!!!! 벡터의 세상 아주 재밌어!!
모르는 것만 살짝 찍먹해보자면
- **PCA(Principal Component Analysis, 주성분 분석):**
  - 데이터가 가장 많이 퍼진 방향(정보 많은 방향) 순으로 축을 새로 잡고, 뒤에 정보 별로 없는 축은 잘라버리는 것
  - 언제 씀?
    - 고차원 피쳐(수백,수천 차원)을 2차원이나 3차원으로 줄여서 **시각화** 할 때
    - 벡터 DB에 넣기 전에 1536 -> 256 차원으로 줄여서 **속도/메모리 줄이기** (성능 희생하는 대신 실용성을 높임)
    - 전통 머신러닝(XGBoost, SVM)에서 **피처가 너무 많을 때 노이즈 줄이고 과적합 방지**
- **SVD (Singular Value Decomposition, 특이값 분해):**
  - 오 캐글에서 많이 본건데, 큰 행렬을 **“요약된 패턴 + 잡소리”**로 나누는 기술임.
    - [유저x아이템], [문서x단어] 이런 행렬을 [핵심 패턴 몇개x그 패턴의 조합] 으로 쪼갠다는데... 대충 숨겨진 축을 찾아내서 그 중에 중요한 것만 남기고 저차원 임베딩을 만드는 수학 도구라고 함.
  - PCA도 사실 SVD의 한 특수 케이스 느낌인 것임!
- **Autoencoder:**
  - 입력 -> 압축 -> 다시 복원 하는 것을 배우는 작은 신경망임.
  - 좁은 병목층에서 자동으로 저차원 표현을 배우는 구조라는데~ 비선형 구조일 때(ReLU, tanh, sigmoid 등) 압축하는 신경만인 느낌
    - 언제 씀?
      - 이미지: 고해상도 이미지를 압축했다가 복원 -> 이미지 압축, 노이즈 제거
      - 이상치 탐지: 정상 데이터로만 학습 -> 나중에 복원 오류가 큰 것을 이상치로 봄
      - 임베딩: 거대한 임베딩/피쳐를 비선형으로 차원 줄여서 벡터 검색, downstream task에 씀
- **Projection (Random Projection):**
  - 고차원 벡터를 랜덤한 저차원 평면에 던져서 대충 누르는 거
  - 적당한 저차원 평면의 차원수만 확보하면 거리/각도가 꽤 잘 보존된다는 듯(Johnson–Lindenstrauss, 존슨-린덴스트라우스 레마 이건 한국어 자료가 잘 없네)
  - 비싼 PCA/SVD 전에 간단한 줄이기로 쓰기도 함
- **ANN(Approximate Nearest Neighbor, 근사치 최근접) 인덱스:**
  - 임베딩 1억개 있는데, 매번 각 쌍에 대해서 거리 재면 어우 장난이 아니니까 **대충 제일 가까운 애들을 엄청 빠르게 찾아주는 인덱스**라고 함.
  - 인덱스를 미리 만들어두고, 가능성이 높은 후보만 빠르게 탐색함. 
  - 약간의 근사 오차가 있어도 속도 향상이 넘 좋다고 함.
  - 언제씀?
    - RAG/검색 시스템: FAISS, HNSWlib, ScaNN, Milvus, Pinecone, Weaviate 등
    - 추천 시스템: 유저 임베딩 -> 비슷한 아이템 top-k
    - 이미지 임베딩 -> 비슷한 이미지 찾기
  - 요즘 벡터DB라고 부르는 서비스들은 까보면 다 ANN엔진이라고 한다!
  